# smg_learning
This repository contains all the important code for the work I have done this year on machine learning for sonomyographical data from the Verasonics system combined with motion tracking data from a Vicon system.
Note: The first attempts at networks used smaller batches that contained only one class and switched between batches frequently because of the extreme memory requirements. This was changed for the final attempts at training but it did not change the results of the training.
# Explanation of Files
## Testing files
### adjust_image.ipynb
This file was used for testing a variety of transforms on image data. To add more transforms to test, simply add some operators into the third and fourth boxes, and then add the name of the variable to graph in the names array in the last box. The loop in the last box plots both the image itself and also the histogram of the data in that frame.
### mse_image_correlation.py
This script was for analyzing the offset of the Vicon and Verasonics data. This was done before we were able to get the trigger system set up for data collection. It does a basic image correlation on the ultrasound images and has offsets that can be used to match up the first frame of the Vicon movement with the first frame of the ultrasound movement.
### simple_cnn.py
This file contains the base class for the Convolutional Neural Net that I used in this project. This was originally developed by another student working on this project for another data set, that we were hoping to adapt to our new data set.
### trigger_info.py
This file shows statistics on trigger timing from the Verasonics system. It shows the maximum and minimum timing between signals for tracking how well the system is able to maintain consistent timing during a trial, and making sure that the data sets from Verasonics and Vicon match up.
## Main files
### cnn.ipynb
This file contains code for a network based off of the one in simple_cnn.py and the code for training and testing it in other notebook cells. This code will look in a specified folder that contains all of your data. The training images and angles should end in '-train-images.npy' and '-train-angles.npy' respectively. The testing images should have the same naming scheme but with 'train' replaced with 'test' in all the file names.
### data_slicer.ipynb
This script slices up the data files into a set number of files that contain each class. This was done to help with managing memory requirements and also allow with multiple classes to be trained on in each batch. This is how the rnn-v2 network processes data, but all other classes work on the individual class files.
### load_us.m
This script transforms the mat files generated by the Verasonics system into csv files that are easily readable by Python
### new_process_vicon.py
This processes the trigger file and the motion capture file to calculate the MCP angles of each finger at each capture frame so that they can be directly matched with the Verasonics captures.
### rnn-v2.ipynb
This is the second attempt at a RNN based network, that is most similar to the one in rnn.ipynb. The main difference is that this one operates on the outputs from the data_slicer script so that it can train on batches that contain all classes at once
### rnn.ipynb
This is my first attempt at a Recurrent Neural Network based system for classifying motion into a particular type such as bottle grab or index finger flexing. This had two separate models based on Long Short Term Memory cells, and one of them had 2 convolutional layers before that layer to see if that could help abstract the data to something more usable by the network.
### splitter.ipynb
This file splits up a data file from a specific class into training and testing data on both images and angles
